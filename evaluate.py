from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

def calculate_perplexity(sentence):
    model_name = "gpt2"
    model = GPT2LMHeadModel.from_pretrained(model_name)
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)

    encodings = tokenizer(sentence, return_tensors="pt")
    with torch.no_grad():
        outputs = model(**encodings, labels=encodings["input_ids"])
    
    loss = outputs.loss
    perplexity = torch.exp(loss).item()
    return perplexity

# Example usage
# generated_sentence = "This is an example sentence generated by my model."
# print("Perplexity:", calculate_perplexity(generated_sentence))